{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id ='zWndV5hUJ8XZYshw1V9axw',\n",
    "                     client_secret ='4ovj54M6J9s4yS6alG7c58jKOa3TQA',\n",
    "                     user_agent ='my user agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = ['bipolar', 'schizophrenia']\n",
    "submissions_types = ['hot', 'controversial', 'new', 'rising', 'top']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to scrape for bipolar subreddit\n",
      "\n",
      "Hot posts from bipolar subreddit scraped\n",
      "Controversial posts from bipolar subreddit scraped\n",
      "New posts from bipolar subreddit scraped\n",
      "Rising posts from bipolar subreddit scraped\n",
      "Top posts from bipolar subreddit scraped\n",
      "Scraping for bipolar subreddit completed successfully\n",
      "Exporting to csv for bipolar subreddit completed successfully\n",
      "----------------------------------------\n",
      "Starting to scrape for schizophrenia subreddit\n",
      "\n",
      "Hot posts from schizophrenia subreddit scraped\n",
      "Controversial posts from schizophrenia subreddit scraped\n",
      "New posts from schizophrenia subreddit scraped\n",
      "Rising posts from schizophrenia subreddit scraped\n",
      "Top posts from schizophrenia subreddit scraped\n",
      "Scraping for schizophrenia subreddit completed successfully\n",
      "Exporting to csv for schizophrenia subreddit completed successfully\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for x in subs:\n",
    "    posts = []\n",
    "    subreddit = reddit.subreddit(x)\n",
    "\n",
    "    print(f'Starting to scrape for {x} subreddit')\n",
    "    print()\n",
    "\n",
    "    for submission_type in submissions_types:\n",
    "        submission_generator = getattr(subreddit, submission_type)(limit=1000)\n",
    "        posts.extend([[submission.id, submission.title, submission.selftext, submission.score, submission.num_comments, submission.author, submission.created_utc, submission.gilded] for submission in submission_generator])\n",
    "        print(f'{submission_type.capitalize()} posts from {x} subreddit scraped')\n",
    "\n",
    "    print(f'Scraping for {x} subreddit completed successfully')\n",
    "\n",
    "    df = pd.DataFrame(posts, columns=['id', 'title', 'text', 'score', 'comments_count', 'author', 'created_utc', 'gilding'])\n",
    "    df.to_csv(f'{x}.csv', index = False)\n",
    "\n",
    "    print(f'Exporting to csv for {x} subreddit completed successfully')\n",
    "    print('----------------------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Ryan's Code\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = ['https://www.reddit.com/r/bipolar.json', 'https://www.reddit.com/r/schizophrenia.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to scrape for bipolar subreddit\n",
      "Sleeping for 6 seconds.\n",
      "Sleeping for 3 seconds.\n",
      "Sleeping for 6 seconds.\n",
      "Sleeping for 2 seconds.\n",
      "Scraping and exporting to csv for bipolar subreddit completed successfully\n",
      "------------------------------\n",
      "Starting to scrape for schizophrenia subreddit\n",
      "Sleeping for 4 seconds.\n",
      "Sleeping for 5 seconds.\n",
      "Sleeping for 4 seconds.\n",
      "Sleeping for 5 seconds.\n",
      "Scraping and exporting to csv for schizophrenia subreddit completed successfully\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for url in url_list:\n",
    "    headers = {'User-agent': 'Pony Inc 1.0'}\n",
    "    posts = []\n",
    "    after = None\n",
    "    \n",
    "    print(f'Starting to scrape for {subs[sub_count]} subreddit')\n",
    "\n",
    "    for a in range(4):\n",
    "        if after:\n",
    "            current_url = f\"{url}?after={after}\"\n",
    "        else:\n",
    "            current_url = url\n",
    "\n",
    "        res = requests.get(current_url, headers=headers)\n",
    "        \n",
    "        if res.status_code != 200:\n",
    "            print('Status error', res.status_code)\n",
    "            break\n",
    "        \n",
    "        current_dict = res.json()\n",
    "        current_posts = [p['data'] for p in current_dict['data']['children']]\n",
    "        posts.extend(current_posts)\n",
    "        after = current_dict['data']['after']\n",
    "        \n",
    "        if a > 0:\n",
    "            prev_posts = pd.read_csv(f'ryan_code_{subs[sub_count]}.csv')\n",
    "            current_df = pd.DataFrame(posts)\n",
    "            combined_df = pd.concat([prev_posts, current_df])\n",
    "            combined_df.to_csv(f'ryan_code_{subs[sub_count]}.csv', index = False)\n",
    "        else:\n",
    "            pd.DataFrame(posts).to_csv(f'ryan_code_{subs[sub_count]}.csv', index = False)\n",
    "\n",
    "        # generate a random sleep duration to look more 'natural'\n",
    "        sleep_duration = random.randint(2,6)\n",
    "        print('Sleeping for', sleep_duration, 'seconds.')\n",
    "        time.sleep(sleep_duration)\n",
    "    \n",
    "    print(f'Scraping and exporting to csv for {subs[sub_count]} subreddit completed successfully')\n",
    "    print('------------------------------')\n",
    "    sub_count +=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
