{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["This is Charles' first try at the modelling of the reddit data"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Importing"]},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.pipeline import Pipeline\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.compose import ColumnTransformer\n","from textblob import TextBlob"]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[],"source":["csv = pd.read_csv(\"../data/combined_cleaned_vectorized.csv\")\n","\n","#make new df with only the text and the label columns\n","df = csv[['text', 'label']].copy()\n","y = df['label']"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Adding features to see the lexical diversity and sentiment of each post"]},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"'int' object has no attribute 'split'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[93], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Adding a feature to see lexical diversity\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df\u001b[39m.\u001b[39mloc[:, \u001b[39m'\u001b[39m\u001b[39mlexical_diversity\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: \u001b[39mlen\u001b[39;49m(\u001b[39mset\u001b[39;49m(x\u001b[39m.\u001b[39;49msplit())) \u001b[39m/\u001b[39;49m \u001b[39mlen\u001b[39;49m(x\u001b[39m.\u001b[39;49msplit()) \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(x\u001b[39m.\u001b[39;49msplit()) \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m \u001b[39m0\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Adding a feature to see sentiment\u001b[39;00m\n\u001b[1;32m      5\u001b[0m df\u001b[39m.\u001b[39mloc[:, \u001b[39m'\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: TextBlob(x)\u001b[39m.\u001b[39msentiment\u001b[39m.\u001b[39mpolarity)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1175\u001b[0m             values,\n\u001b[1;32m   1176\u001b[0m             f,\n\u001b[1;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1178\u001b[0m         )\n\u001b[1;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n","File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n","Cell \u001b[0;32mIn[93], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Adding a feature to see lexical diversity\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df\u001b[39m.\u001b[39mloc[:, \u001b[39m'\u001b[39m\u001b[39mlexical_diversity\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m(x\u001b[39m.\u001b[39msplit())) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(x\u001b[39m.\u001b[39msplit()) \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(x\u001b[39m.\u001b[39;49msplit()) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Adding a feature to see sentiment\u001b[39;00m\n\u001b[1;32m      5\u001b[0m df\u001b[39m.\u001b[39mloc[:, \u001b[39m'\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: TextBlob(x)\u001b[39m.\u001b[39msentiment\u001b[39m.\u001b[39mpolarity)\n","\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'split'"]}],"source":["# Adding a feature to see lexical diversity\n","df.loc[:, 'lexical_diversity'] = df['text'].apply(lambda x: len(set(x.split())) / len(x.split()) if len(x.split()) > 0 else 0)\n","\n","# Adding a feature to see sentiment\n","df.loc[:, 'sentiment'] = df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Topic Modelling Should also be added as a feature"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import gensim\n","from gensim import corpora\n","from gensim.models import LdaModel\n","\n","# Create a dictionary from the data\n","dictionary = corpora.Dictionary(df['text'].apply(lambda x: x.split()))\n","\n","# Create a bag of words corpus by passing the tokenized list of words to the dictionary\n","corpus = [dictionary.doc2bow(text) for text in df['text'].apply(lambda x: x.split())]\n","\n","# Initialise the LDA model and fit\n","num_topics = 10  # change this based on your understanding of the data\n","lda = LdaModel(corpus, num_topics=num_topics, id2word=dictionary)\n","\n","# get the topic distribution of each document and take the most probable topic as label\n","df.loc[:, 'topic'] = df['text'].apply(lambda x: sorted(lda[dictionary.doc2bow(x.split())], key=lambda item: -item[1])[0][0])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['topic'] = df['topic'].astype('category')\n","topic_dummies = pd.get_dummies(df['topic'], prefix='topic')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.concat([df, topic_dummies], axis=1)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Finally, use word vectorization to improve the model some more"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# To include this feature in your model, you'll need to adjust your train/test split:\n","X = df[['text', 'lexical_diversity', 'sentiment', 'topic']]\n","y = df['label']\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(df.head())"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Train and Test Splitting"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Split the dataset into train and test\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Excluding Words from Dataset and Converting text into token counts"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define custom stop words\n","custom_stop_words = []"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert text into matrix of token counts and then transform a count matrix to a normalized tf-idf representation\n","pipeline = Pipeline([\n","    ('vect', CountVectorizer(ngram_range=(1, 2), stop_words=custom_stop_words)),\n","    ('tfidf', TfidfTransformer()),\n","])\n","\n","X_train_transformed = pipeline.fit_transform(X_train['text'])\n","X_test_transformed = pipeline.transform(X_test['text'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train_df = pd.DataFrame(X_train_transformed.toarray(),\n","    columns=pipeline['vect'].get_feature_names_out())\n","X_train_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Define your text processing pipeline\n","text_pipeline = Pipeline([\n","    ('vect', CountVectorizer(ngram_range=(1), stop_words=custom_stop_words)),\n","    ('tfidf', TfidfTransformer()),\n","])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('text', text_pipeline, 'text'),\n","        ('num', StandardScaler(), ['lexical_diversity','sentiment']),\n","        ('topic', 'passthrough', topic_dummies.columns),\n","    ])\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Modelling"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Initialize models\n","clf1 = RandomForestClassifier(n_estimators=1000, random_state=42)\n","clf2 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3), n_estimators=100)\n","clf3 = GradientBoostingClassifier(n_estimators=1000, learning_rate=1.0, max_depth=3, random_state=42)\n","\n","# Create ensemble model\n","eclf = VotingClassifier(estimators=[('rf', clf1), ('adb', clf2), ('gb', clf3)], voting='hard')\n","\n","for clf, label in zip([clf1, clf2, clf3, eclf], ['Random Forest', 'AdaBoost', 'GradientBoost', 'Ensemble']):\n","    clf.fit(X_train_transformed, y_train)\n","    print(\"Accuracy: %0.2f [%s]\" % (clf.score(X_test_transformed, y_test), label))\n"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
